## Get Started

- Build llama.cpp
- Download models
- Run experiments

### 1. Clone the repository with submodules
```bash
git clone --recurse-submodules https://github.com/jfflorez/Local-LLM-Insights.git
cd Local-LLM-Insights
```

### 2. Download quantized models in GGUF format
``` PowerShell
curl.exe -L -o models/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf
```

``` PowerShell
curl.exe -L -o models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
```
``` PowerShell
curl.exe -L -o models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
```


services:
  llama-server:
    image: local/llama.cpp:full-cpu   # or server-cpu if you built it
    build:
      context: . # denotes parent folder : so both llama.cpp and llama.cpp.myupdates are avaialble
      dockerfile: .devops/cpu.Dockerfile 
      # Comment line above and try line below if your docker build fails on Windows due to eol characters in tools.sh
      #dockerfile: llama.cpp.myupdates/.devops/cpu.Dockerfile
      target: server
    ports:
      - "8080:8080"
    volumes:
      - "C:/llama/models:/models"
    command: >
      -s
      -m /models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
